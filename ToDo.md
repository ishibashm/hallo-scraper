# ハローワーク求人情報スクレイピング ToDoリスト

現在のプロジェクトの進捗と今後のタスクを管理します。

## 完了済みタスク

- [x] **基本設定と環境構築:**
    - [x] プロジェクトフォルダ構成の決定 (`src`, `output`, `config`)。
    - [x] Python仮想環境 (`.venv`) の構築とアクティベート手順の確立。
    - [x] 必要なライブラリ (`selenium`, `beautifulsoup4`, `pandas`, `chromedriver-autoinstaller` 等) の特定と `requirements.txt` への記載準備 (※ファイル自体は未確認)。
    - [x] 設定ファイル (`config/settings.py`) の作成と基本設定 (URL, User-Agent, 出力設定等) の定義。
    - [x] ログ出力設定の実装 (`src/scraper.py`)。
    - [x] `README.md` の作成（概要、セットアップ、実行方法）。
    - [x] 未使用の仮想環境フォルダ (`venv`) の削除。
- [x] **求人リストページのスクレイピング実装:**
    - [x] Selenium WebDriver のセットアップ (`chromedriver-autoinstaller` 利用)。
    - [x] 指定都道府県・求人区分での検索実行。
    - [x] 求人リストページのHTML取得とパース (BeautifulSoup)。
    - [x] リストページのデータ項目抽出 (CSSセレクタの特定と修正含む)。
        - [x] 職種、受付日、紹介期限日
        - [x] 求人区分、事業所名、就業場所、仕事内容、雇用形態、賃金、就業時間、休日、年齢、求人番号、公開範囲
        - [x] こだわり条件 (special_notes_labels)
        - [x] 求人数 (number_of_positions)
    - [x] 抽出データのDataFrameへの格納 (Pandas)。
    - [x] データのCSVファイルへの保存 (`output` ディレクトリ)。
    - [x] ファイル命名規則の実装。
    - [x] 基本的なエラーハンドリング (WebDriverセットアップ失敗、ページ遷移失敗など)。
    - [x] 次ページ存在チェック機能の実装 (`check_next_page_exists`)。
    - [x] コマンドライン引数による都道府県コード・ページ番号指定機能の実装。
- [x] **問題解決とデバッグ:**
    - [x] 求人区分選択漏れによる検索失敗の修正。
    - [x] `ModuleNotFoundError` の解決 (仮想環境利用の徹底)。
    - [x] CSSセレクタの不一致によるデータ抽出失敗の特定と修正 (デバッグログ、HTMLソース分析経由)。
    - [x] Pythonコードのインデントエラー修正。
    - [x] リストページ解析時に1件しか求人を検出できない問題の修正 (`tr.kyujin_head` セレクタの修正)。

## 今後のタスク

- [ ] **ページネーションの実装:**
    - [ ] `run_scraper_for_page` をループさせ、指定された範囲または全ページのリストデータを連続して取得する機能。
    - [ ] `check_next_page_exists` の結果を利用してループを制御する。
- [ ] **求人詳細ページのスクレイピング実装 (疎結合設計):**
    - [ ] **1. 詳細ページ構造分析とセレクタ定義:**
        - [ ] 詳細ページのHTML構造を分析し、抽出したいデータ項目（電話番号、HP、資本金、企業規模、従業員数、事業内容等）をリストアップする。
        - [ ] 各データ項目のCSSセレクタを特定し、`config/settings.py` に `DETAIL_SELECTORS` として定義する。
    - [ ] **2. 詳細リンク抽出:**
        - [ ] `src/scraper.py` (`HelloWorkScraper`) を修正し、一覧ページ解析時に「詳細を表示」ボタンの `href` 属性を抽出する。
        - [ ] 抽出した詳細リンク（またはパラメータ）を求人番号と共に一時ファイル（例：`output/detail_links.json`）に保存するロジックを追加。
    - [ ] **3. 詳細スクレイパー (`DetailScraper`) の実装:**
        - [ ] 新しいファイル `src/detail_scraper.py` を作成し、`DetailScraper` クラスを定義する。
        - [ ] `DetailScraper` に一時ファイルを読み込む機能を追加する。
        - [ ] Selenium WebDriver をセットアップする機能を追加する (`HelloWorkScraper` と同様の処理）。
        - [ ] 詳細ページに遷移し、HTMLを取得するメソッド (`fetch_detail_page`) を実装する。
        - [ ] `config/settings.py` の `DETAIL_SELECTORS` を使用して、詳細ページからデータを抽出するメソッド (`parse_detail_page`) を実装する。
        - [ ] 抽出した詳細データを独立したファイル（例：`output/job_details_[求人番号].csv` またはまとめて `output/job_details.csv`）に保存する機能を追加する。
        - [ ] 詳細ページ取得時のエラーハンドリング（タイムアウト、404など）を実装する。
    - [ ] **4. 実行フローの統合:**
        - [ ] `src/scraper.py` のメイン実行ブロック (`if __name__ == "__main__":`) にコマンドライン引数 `--fetch-details` を追加する。
        - [ ] `--fetch-details` が指定された場合、一覧取得後に `DetailScraper` を呼び出すロジックを追加する。
    - [ ] **5. データ結合 (オプション):**
        - [x] 必要であれば、一覧データと詳細データを結合するための別スクリプト (`src/merge_data.py`) を作成する。 (`python src/merge_data.py <list_csv> <detail_csv> <output_csv>`)
- [ ] **エラーハンドリング強化:**
    - [ ] ネットワークエラー、タイムアウト、予期せぬページ構造変化などに対するリトライ処理や、より詳細なエラーログ出力。
    - [ ] データ抽出失敗時の代替処理や警告レベルの見直し。
- [ ] **テスト拡充:**
    - [ ] 各関数の単体テストを作成する (例: `unittest`, `pytest`)。
    - [ ] ダミーのHTMLファイルを用いたパース処理のテスト。
    - [ ] 結合テストシナリオの拡充。
- [ ] **コードの最適化とリファクタリング:**
    - [ ] 冗長なコードの削減。
    - [ ] 関数の責務分割の見直し。
    - [ ] コメントやドキュメンテーションの充実。
- [ ] **`requirements.txt` の確定:**
    - [ ] 現在インストールされているライブラリを正確に反映させる (`pip freeze > requirements.txt`)。
- [ ] **運用・保守フェーズの検討:** (フェーズ4)
    - [ ] 定期実行やログ監視の方法。
    - [ ] サイト変更への対応フロー。

## 注意点

- **法的・倫理的責任:** スクレイピングは対象サイトの規約と法律を遵守し、サーバーに負荷をかけすぎないよう、常に倫理的に行ってください (`REQUEST_INTERVAL` を適切に設定)。
- **サイト変更:** ウェブサイトの構造は予告なく変更される可能性があります。スクリプトが動作しなくなった場合は、HTML構造や通信パラメータの変更を確認し、スクリプト (`src/scraper.py`) やセレクタ (`config/settings.py`) を修正する必要があります。
- **情報公開範囲:** スクレイピングで取得できるのは、あくまでサイト上で公開されている情報のみです。

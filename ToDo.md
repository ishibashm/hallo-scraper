# ハローワーク求人情報スクレイピング ToDoリスト

現在のプロジェクトの進捗と今後のタスクを管理します。

## 完了済みタスク

- [x] **基本設定と環境構築:**
    - [x] プロジェクトフォルダ構成の決定 (`src`, `output`, `config`)。
    - [x] Python仮想環境 (`.venv`) の構築とアクティベート手順の確立。
    - [x] 必要なライブラリ (`selenium`, `beautifulsoup4`, `pandas`, `chromedriver-autoinstaller` 等) の特定と `requirements.txt` への記載準備 (※ファイル自体は未確認)。
    - [x] 設定ファイル (`config/settings.py`) の作成と基本設定 (URL, User-Agent, 出力設定等) の定義。
    - [x] ログ出力設定の実装 (`src/scraper.py`)。
    - [x] `README.md` の作成（概要、セットアップ、実行方法）。
    - [x] 未使用の仮想環境フォルダ (`venv`) の削除。
- [x] **求人リストページのスクレイピング実装:**
    - [x] Selenium WebDriver のセットアップ (`chromedriver-autoinstaller` 利用)。
    - [x] 指定都道府県・求人区分での検索実行。
    - [x] 求人リストページのHTML取得とパース (BeautifulSoup)。
    - [x] リストページのデータ項目抽出 (CSSセレクタの特定と修正含む)。
        - [x] 職種、受付日、紹介期限日
        - [x] 求人区分、事業所名、就業場所、仕事内容、雇用形態、賃金、就業時間、休日、年齢、求人番号、公開範囲
        - [x] こだわり条件 (special_notes_labels)
        - [x] 求人数 (number_of_positions)
    - [x] 抽出データのDataFrameへの格納 (Pandas)。
    - [x] データのCSVファイルへの保存 (`output` ディレクトリ)。
    - [x] ファイル命名規則の実装。
    - [x] 基本的なエラーハンドリング (WebDriverセットアップ失敗、ページ遷移失敗など)。
    - [x] 次ページ存在チェック機能の実装 (`check_next_page_exists`)。
    - [x] コマンドライン引数による都道府県コード・ページ番号指定機能の実装。
- [x] **問題解決とデバッグ:**
    - [x] 求人区分選択漏れによる検索失敗の修正。
    - [x] `ModuleNotFoundError` の解決 (仮想環境利用の徹底)。
    - [x] CSSセレクタの不一致によるデータ抽出失敗の特定と修正 (デバッグログ、HTMLソース分析経由)。
    - [x] Pythonコードのインデントエラー修正。

## 今後のタスク

- [ ] **ページネーションの実装:**
    - [ ] `run_scraper_for_page` をループさせ、指定された範囲または全ページのリストデータを連続して取得する機能。
    - [ ] `check_next_page_exists` の結果を利用してループを制御する。
- [ ] **求人詳細ページのスクレイピング実装:**
    - [ ] リストページから詳細ページへのリンク (または必要なパラメータ) を取得するロジック。
    - [ ] 詳細ページのHTMLを取得する機能。
    - [ ] 詳細ページのHTML構造を分析し、追加情報 (電話番号、HP、資本金など `DETAIL_SELECTORS` で定義予定の項目) のCSSセレクタを特定・定義 (`config/settings.py`)。
    - [ ] 詳細ページからデータを抽出するロジックを実装 (`parse_detail_page_data` のような関数)。
    - [ ] 抽出した詳細データを既存のリストデータと結合する。
- [ ] **エラーハンドリング強化:**
    - [ ] ネットワークエラー、タイムアウト、予期せぬページ構造変化などに対するリトライ処理や、より詳細なエラーログ出力。
    - [ ] データ抽出失敗時の代替処理や警告レベルの見直し。
- [ ] **テスト拡充:**
    - [ ] 各関数の単体テストを作成する (例: `unittest`, `pytest`)。
    - [ ] ダミーのHTMLファイルを用いたパース処理のテスト。
    - [ ] 結合テストシナリオの拡充。
- [ ] **コードの最適化とリファクタリング:**
    - [ ] 冗長なコードの削減。
    - [ ] 関数の責務分割の見直し。
    - [ ] コメントやドキュメンテーションの充実。
- [ ] **`requirements.txt` の確定:**
    - [ ] 現在インストールされているライブラリを正確に反映させる (`pip freeze > requirements.txt`)。
- [ ] **運用・保守フェーズの検討:** (フェーズ4)
    - [ ] 定期実行やログ監視の方法。
    - [ ] サイト変更への対応フロー。

## 注意点

- **法的・倫理的責任:** スクレイピングは対象サイトの規約と法律を遵守し、サーバーに負荷をかけすぎないよう、常に倫理的に行ってください (`REQUEST_INTERVAL` を適切に設定)。
- **サイト変更:** ウェブサイトの構造は予告なく変更される可能性があります。スクリプトが動作しなくなった場合は、HTML構造や通信パラメータの変更を確認し、スクリプト (`src/scraper.py`) やセレクタ (`config/settings.py`) を修正する必要があります。
- **情報公開範囲:** スクレイピングで取得できるのは、あくまでサイト上で公開されている情報のみです。
